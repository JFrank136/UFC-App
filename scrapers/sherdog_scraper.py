import os, sys, re, json, time, threading, asyncio
from pathlib import Path
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import aiohttp

# Dynamically add the utils directory to the path
utils_path = Path(__file__).resolve().parent / "utils"
sys.path.append(str(utils_path))

from name_fixes import URL_OVERRIDES

HEADERS = {"User-Agent": "Mozilla/5.0"}
OUTPUT_FILE = "data/sherdog_fighters.json"

# Thread-safe session with connection pooling
class SessionManager:
    def __init__(self, max_workers=10):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        # Connection pooling
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=20,
            pool_maxsize=20,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        self.lock = threading.Lock()
    
    def get(self, url, **kwargs):
        return self.session.get(url, **kwargs)

# Global session manager
session_manager = SessionManager()

def search_sherdog(fighter_name):
    """Uncached version of search"""
    return search_sherdog_internal(fighter_name)


def search_sherdog_internal(fighter_name):
    # Check if this name has a direct override
    override = URL_OVERRIDES.get(fighter_name.upper())
    if override:
        print(f"üîó Using URL override for {fighter_name}")
        return override

    # Default search behavior
    query = fighter_name.replace(" ", "+")
    url = f"https://www.sherdog.com/stats/fightfinder?SearchTxt={query}"
    
    try:
        res = session_manager.get(url, timeout=10)
        if res.status_code != 200:
            return None
            
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Get all fighter links from search results
        links = soup.select("table.fightfinder_result a[href^='/fighter/']")
        if not links:
            return None
        
        # If only one result, return it
        if len(links) == 1:
            return "https://www.sherdog.com" + links[0]['href']
        
        # Multiple results - find the best match using simple string comparison
        best_match = None
        best_score = 0
        
        for link in links:
            # Get the fighter name from the link text
            result_name = link.text.strip()
            
            # Simple similarity calculation (case-insensitive)
            name_lower = fighter_name.lower()
            result_lower = result_name.lower()
            
            # Exact match gets highest priority
            if name_lower == result_lower:
                print(f"  ‚úÖ Exact match found: {result_name}")
                return "https://www.sherdog.com" + link['href']
            
            # Check if search name is contained in result or vice versa
            if name_lower in result_lower or result_lower in name_lower:
                # Calculate simple similarity score based on length difference
                score = min(len(name_lower), len(result_lower)) / max(len(name_lower), len(result_lower)) * 100
                
                print(f"  Comparing '{fighter_name}' vs '{result_name}' - Score: {score:.1f}")
                
                if score > best_score:
                    best_score = score
                    best_match = link
        
        # Only return if we have a reasonable match (>= 70% similarity)
        if best_match and best_score >= 70:
            print(f"  ‚úÖ Best match: {best_match.text.strip()} (Score: {best_score:.1f})")
            return "https://www.sherdog.com" + best_match['href']
        else:
            print(f"  ‚ùå No good match found. Best score: {best_score:.1f}")
            # Fall back to first result if we can't find a good match
            print(f"  üîÑ Falling back to first result: {links[0].text.strip()}")
            return "https://www.sherdog.com" + links[0]['href']
            
    except Exception as e:
        print(f"‚ö†Ô∏è Search error for {fighter_name}: {e}")
    
    return None


def scrape_fighter(url):
    try:
        res = session_manager.get(url, timeout=15)
        if res.status_code != 200:
            print(f"‚ùå Failed to load: {url}")
            return None

        soup = BeautifulSoup(res.text, "html.parser")

        # Nationality
        nationality_tag = soup.select_one("strong[itemprop='nationality']")
        country = nationality_tag.text.strip() if nationality_tag else None

        # Age - optimized regex
        age = "Unknown"
        age_row = soup.find("td", string=re.compile(r"^\s*AGE\s*$", re.IGNORECASE))
        if age_row and age_row.find_next_sibling("td"):
            age_text = age_row.find_next_sibling("td").get_text(" ", strip=True)
            match = re.search(r"\d{1,3}", age_text)
            if match:
                age = match.group()

        # Weight Class
        weight_class = None
        weight_block = soup.select_one("div.association-class a[href*='weightclass']")
        if weight_block:
            weight_class = weight_block.text.strip()

        # Wins / Losses total - Improved extraction
        def extract_total(section_name):
            try:
                if section_name == "wins":
                    win_div = soup.select_one("div.winloses.win span:nth-of-type(2)")
                    return win_div.text.strip() if win_div else "0"
                elif section_name == "losses":
                    loss_div = soup.select_one("div.winloses.lose span:nth-of-type(2)")
                    return loss_div.text.strip() if loss_div else "0"
            except Exception as e:
                print(f"‚ö†Ô∏è Error extracting {section_name}: {e}")

            # Fallback to breakdown if the above fails
            if section_name == "wins":
                ko = int(wins_by.get("KOS/TKOS", "0"))
                sub = int(wins_by.get("SUBMISSIONS", "0"))
                dec = int(wins_by.get("DECISIONS", "0"))
                return str(ko + sub + dec)
            elif section_name == "losses":
                ko = int(losses_by.get("KOS/TKOS", "0"))
                sub = int(losses_by.get("SUBMISSIONS", "0"))
                dec = int(losses_by.get("DECISIONS", "0"))
                return str(ko + sub + dec)

            return "0"

        # KO/Sub/Dec breakdown - Improved with better fallbacks
        def parse_method_breakdown(section_name):
            result = {"KOS/TKOS": "0", "SUBMISSIONS": "0", "DECISIONS": "0"}
            section = soup.select_one(f"div.{section_name}")
            if not section:
                return result

            # Loop over each "meter-title" and find the next .meter div for its value
            for meter_title in section.select("div.meter-title"):
                label = meter_title.get_text(strip=True).upper()
                meter_div = meter_title.find_next_sibling("div", class_="meter")
                if not meter_div:
                    continue
                value_div = meter_div.select_one("div.pl")
                value = value_div.get_text(strip=True) if value_div else "0"

                if "KO" in label:
                    result["KOS/TKOS"] = value
                elif "SUBMISSION" in label:
                    result["SUBMISSIONS"] = value
                elif "DECISION" in label:
                    result["DECISIONS"] = value

            return result

        wins_by = parse_method_breakdown("wins")
        losses_by = parse_method_breakdown("loses")
        
        wins_total = extract_total("wins")
        losses_total = extract_total("losses")

        summary = {
            "country": country,
            "age": age,
            "weight_class": weight_class,
            "Wins": wins_total,
            "Losses": losses_total,
            "wins_by_ko": wins_by.get("KOS/TKOS"),
            "wins_by_sub": wins_by.get("SUBMISSIONS"),
            "wins_by_dec": wins_by.get("DECISIONS"),
            "losses_by_ko": losses_by.get("KOS/TKOS"),
            "losses_by_sub": losses_by.get("SUBMISSIONS"),
            "losses_by_dec": losses_by.get("DECISIONS"),
        }

        # Fight History - optimized with list comprehension where possible
        history = []
        rows = soup.select("table.new_table.fighter tr")
        
        # Pre-compile regex for better performance
        date_pattern = re.compile(r"[A-Za-z]{3} / \d{2} / \d{4}")
        method_split_pattern = re.compile(r'\n|\s{2,}|[A-Z][a-z]+ [A-Z][a-z]+')
        
        for row in rows:
            cols = row.select("td")
            if len(cols) < 6:
                continue
            
            # Skip header rows
            first_col_text = cols[0].text.strip().lower()
            if any(hdr in first_col_text for hdr in ["result", "fighter"]):
                continue

            raw_date = cols[2].text.strip()
            date_match = date_pattern.search(raw_date)
            if date_match:
                try:
                    dt = datetime.strptime(date_match.group(), "%b / %d / %Y")
                    clean_date = dt.strftime("%m/%d/%Y")
                except ValueError:
                    clean_date = raw_date
            else:
                clean_date = raw_date

            method_raw = cols[3].text.strip()
            method_clean = method_split_pattern.split(method_raw)[0].strip()
            if not method_clean.endswith(")") and "(" in method_clean:
                method_clean += ")"

            fight = {
                "result": cols[0].text.strip().lower(),
                "fighter": cols[1].text.strip(),
                "fight_date": clean_date,
                "method": method_clean,
                "round": cols[4].text.strip(),
                "time": cols[5].text.strip(),
            }
            history.append(fight)

        name_tag = soup.select_one("span.fn")
        name = name_tag.text.strip() if name_tag else url.split("/")[-1]

        nickname_tag = soup.select_one("span.nickname")
        nickname = nickname_tag.get_text(strip=True).strip('"') if nickname_tag else None

        import uuid

        return {
            "id": str(uuid.uuid4()),
            "name": name,
            "nickname": nickname,
            "profile_url_sherdog": url,
            "country": country,
            "age": age,
            "weight_class": weight_class,
            "wins_total": wins_total,
            "losses_total": losses_total,
            "wins_ko": wins_by.get("KOS/TKOS"),
            "wins_sub": wins_by.get("SUBMISSIONS"),
            "wins_dec": wins_by.get("DECISIONS"),
            "losses_ko": losses_by.get("KOS/TKOS"),
            "losses_sub": losses_by.get("SUBMISSIONS"),
            "losses_dec": losses_by.get("DECISIONS"),
            "fight_history": history  # optional ‚Äî can remove this too
        }


    except Exception as e:
        print(f"‚ö†Ô∏è Error scraping {url}: {e}")
        return None

def save_progress(data):
    """Thread-safe save function"""
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with session_manager.lock:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

def process_fighter(fighter_info):
    """Process a single fighter - designed for threading"""
    idx, total, fighter = fighter_info
    name = fighter.get("name")
    
    print(f"[{idx}/{total}] Searching for {name} on Sherdog...")
    profile_url = search_sherdog(name)
    
    if not profile_url:
        print(f"‚ùå No Sherdog profile found for: {name}")
        return None, {"name": name, "reason": "not found in search"}

    print(f"‚Üí Scraping profile: {profile_url}")
    fighter_data = scrape_fighter(profile_url)
    
    if fighter_data:
        print(f"‚úÖ Scraped: {fighter_data['name']}")
        return fighter_data, None
    else:
        print(f"‚ùå Failed to scrape data from profile: {profile_url}")
        return None, {"name": name, "url": profile_url, "reason": "scrape failed"}

def main():
    with open("data/ufc_fighters_raw.json", "r", encoding="utf-8") as f:
        roster = json.load(f)

    active_fighters = [f for f in roster if f.get("status", "").lower() == "active"]

    results = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            results = json.load(f)

    fighters_to_scrape = [
        (idx, len(active_fighters), fighter)
        for idx, fighter in enumerate(active_fighters, start=1)
    ]

    
    if not fighters_to_scrape:
        print("No new fighters to scrape!")
        return

    failures = []
    
    # Use ThreadPoolExecutor for concurrent processing
    max_workers = min(8, len(fighters_to_scrape))  # Limit concurrent requests
    
    print(f"üöÄ Starting concurrent scraping with {max_workers} workers...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_fighter = {
            executor.submit(process_fighter, fighter_info): fighter_info 
            for fighter_info in fighters_to_scrape
        }
        
        # Process completed tasks as they finish
        for future in as_completed(future_to_fighter):
            try:
                fighter_data, failure = future.result()
                
                if fighter_data:
                    results.append(fighter_data)
                    # Save progress every 10 successful scrapes
                    if len(results) % 10 == 0:
                        save_progress(results)
                        
                if failure:
                    failures.append(failure)
                    
            except Exception as e:
                fighter_info = future_to_fighter[future]
                print(f"‚ö†Ô∏è Exception processing {fighter_info[2].get('name')}: {e}")
                failures.append({
                    "name": fighter_info[2].get('name'), 
                    "reason": f"exception: {e}"
                })
            
            # Small delay to be respectful
            time.sleep(0.1)

    # Final save
    save_progress(results)
    
    if failures:
        os.makedirs(os.path.dirname("data/errors/sherdog_failures.json"), exist_ok=True)
        with open("data/errors/sherdog_failures.json", "w", encoding="utf-8") as f:
            json.dump(failures, f, indent=2, ensure_ascii=False)
        print(f"‚ö†Ô∏è Logged {len(failures)} failures to errors/sherdog_failures.json")

    print(f"\nüéØ Done. Scraped {len(results)} fighters. Data saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()

    # Auto-run retry script after main scraper
    print("\nüîÅ Running retry_sherdog.py to handle failed scrapes...")
    import subprocess
    subprocess.run(["python", "retry_sherdog.py"])